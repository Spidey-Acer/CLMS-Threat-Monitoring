{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4092f031",
   "metadata": {},
   "source": [
    "# Centralized Log Management System for Threat Monitoring (UNSW-NB15)\n",
    "\n",
    "**Author:** Your Name Here  \n",
    "**Date:** 2025-08-13\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements a memory-efficient, production-minded **Centralized Log Management System (CLMS)** for threat monitoring using the **UNSW-NB15** dataset. It covers:\n",
    "\n",
    "- Automated dataset fetch + robust failure handling\n",
    "- Chunked ingestion with dtype optimization (<= 8GB RAM friendly)\n",
    "- Preprocessing & feature engineering (categoricals + numeric scaling)\n",
    "- EDA with professional plots (class distribution, correlations, feature importance)\n",
    "- Modeling: **Baseline** (Incremental Logistic Regression via `SGDClassifier`) and **Optimized** (Scikit-learn `HistGradientBoostingClassifier`; optional XGBoost if installed)\n",
    "- Full evaluation: Accuracy, Precision, Recall, F1, Confusion Matrix, ROC & AUC\n",
    "- **Centralized log pipeline simulation** for near-real-time scoring and alerting\n",
    "- Clear, modular code for integration into production\n",
    "\n",
    "> **Dataset**: [UNSW-NB15](https://research.unsw.edu.au/projects/unsw-nb15-dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5a2a1",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Modern security operations require collecting logs from multiple sources (network, system, application), centralizing them, and applying analytics to detect threats in real-time or near-real-time.\n",
    "This notebook provides an end-to-end, memory-conscious blueprint you can adapt to your environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab42ad",
   "metadata": {},
   "source": [
    "## 2. Setup & Configuration\n",
    "\n",
    "- Paths and memory limits\n",
    "- Download mirrors for UNSW-NB15 (graceful handling when offline)\n",
    "- Lightweight dependencies by default (sklearn/matplotlib/seaborn); optional XGBoost if installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a6ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Configuration ===\n",
    "from pathlib import Path\n",
    "import os, sys, shutil, hashlib, warnings, io, zipfile, tarfile, gzip, bz2, lzma, tempfile, json\n",
    "from typing import Iterator, Tuple, Dict, Any, List, Optional\n",
    "\n",
    "DATA_DIR = Path(\"./data_unsw_nb15\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Known filenames for the split CSVs commonly distributed\n",
    "TRAIN_CSV = DATA_DIR / \"UNSW_NB15_training-set.csv\"\n",
    "TEST_CSV  = DATA_DIR / \"UNSW_NB15_testing-set.csv\"\n",
    "\n",
    "# Chunk size for memory-conscious loading\n",
    "CHUNK_SIZE = 100_000  # adjust if you have more/less memory\n",
    "\n",
    "# For EDA sampling to keep plots snappy\n",
    "EDA_SAMPLE_N = 100_000\n",
    "\n",
    "# For full-model training; you can reduce to cap memory/time\n",
    "MAX_TRAIN_ROWS = 500_000  # set None to use all training rows\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = os.cpu_count() or 2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Config OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421dcc7",
   "metadata": {},
   "source": [
    "## 3. Dataset Overview & Download\n",
    "\n",
    "The **UNSW-NB15** dataset contains realistic network traffic with labeled normal and attack events across multiple categories.\n",
    "We'll attempt to download the **training** and **testing** CSVs if they're not already present. The code gracefully handles:\n",
    "\n",
    "- Network failures\n",
    "- Missing files\n",
    "- Corrupted archives\n",
    "\n",
    "> If auto-download fails (e.g., offline), place the CSVs in `./data_unsw_nb15/` and re-run the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a8f1000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting download: https://raw.githubusercontent.com/duangenquan/UNSW-NB15/master/UNSW_NB15_training-set.csv\n",
      "[WARN] Download failed from https://raw.githubusercontent.com/duangenquan/UNSW-NB15/master/UNSW_NB15_training-set.csv: HTTP Error 404: Not Found\n",
      "Attempting download: https://raw.githubusercontent.com/duangenquan/UNSW-NB15/master/UNSW_NB15_testing-set.csv\n",
      "[WARN] Download failed from https://raw.githubusercontent.com/duangenquan/UNSW-NB15/master/UNSW_NB15_testing-set.csv: HTTP Error 404: Not Found\n",
      "\n",
      "[NOTICE] Could not auto-download UNSW-NB15 CSVs.\n",
      "Please download the following files and place them in ./data_unsw_nb15/:\n",
      "  - UNSW_NB15_training-set.csv\n",
      "  - UNSW_NB15_testing-set.csv\n",
      "Source: https://research.unsw.edu.au/projects/unsw-nb15-dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib.request\n",
    "\n",
    "def try_download(url: str, dest: Path, timeout: int = 60) -> bool:\n",
    "    \"\"\"Best-effort file download with graceful failure.\"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting download: {url}\")\n",
    "        with urllib.request.urlopen(url, timeout=timeout) as r, open(dest, 'wb') as f:\n",
    "            shutil.copyfileobj(r, f)\n",
    "        print(f\"Downloaded -> {dest}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Download failed from {url}: {e}\")\n",
    "        return False\n",
    "\n",
    "def ensure_unsw_nb15_present() -> None:\n",
    "    \"\"\"Ensure UNSW NB15 CSVs exist; attempt multiple mirrors if needed.\"\"\"\n",
    "    if TRAIN_CSV.exists() and TEST_CSV.exists():\n",
    "        print(\"UNSW-NB15 CSVs already present.\")\n",
    "        return\n",
    "\n",
    "    mirrors = [\n",
    "        # --- Add or update mirrors as needed ---\n",
    "        # Official project page often links to mirrors; direct CSV links can change.\n",
    "        # The following are commonly seen mirror paths. If these fail, place files manually.\n",
    "        # These URLs are examples/placeholders that may or may not be live at runtime.\n",
    "        # Replace or add mirrors in the list as needed.\n",
    "        \"https://raw.githubusercontent.com/duangenquan/UNSW-NB15/master/UNSW_NB15_training-set.csv\",\n",
    "        \"https://raw.githubusercontent.com/duangenquan/UNSW-NB15/master/UNSW_NB15_testing-set.csv\",\n",
    "    ]\n",
    "    ok = False\n",
    "    # Try to download train/test if missing; iterate mirrors in pairs when possible.\n",
    "    # This is a simple strategy: try each mirror for both train and test if needed.\n",
    "    for base in set([m.rsplit('/', 1)[0] for m in mirrors]):\n",
    "        train_url = base + \"/UNSW_NB15_training-set.csv\"\n",
    "        test_url  = base + \"/UNSW_NB15_testing-set.csv\"\n",
    "        got_train = TRAIN_CSV.exists() or try_download(train_url, TRAIN_CSV)\n",
    "        got_test  = TEST_CSV.exists()  or try_download(test_url, TEST_CSV)\n",
    "        if got_train and got_test:\n",
    "            ok = True\n",
    "            break\n",
    "\n",
    "    if not ok:\n",
    "        print(\"\\n[NOTICE] Could not auto-download UNSW-NB15 CSVs.\\n\"\n",
    "              \"Please download the following files and place them in ./data_unsw_nb15/:\\n\"\n",
    "              \"  - UNSW_NB15_training-set.csv\\n  - UNSW_NB15_testing-set.csv\\n\"\n",
    "              \"Source: https://research.unsw.edu.au/projects/unsw-nb15-dataset\\n\")\n",
    "\n",
    "ensure_unsw_nb15_present()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c22e2",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load CSVs using **dtype optimization** and **chunked reading** to minimize memory use.\n",
    "- Handle missing values.\n",
    "- Encode categorical features using an **OrdinalEncoder** (lightweight; stable in streaming).\n",
    "- Standardize numeric features for models that benefit from scaling.\n",
    "\n",
    "We keep preprocessing modular and production-friendly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa6b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing utilities ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "LABEL_COL = 'label'       # 0=normal, 1=attack in UNSW split CSVs\n",
    "ATTACK_CAT = 'attack_cat' # high-level attack category\n",
    "\n",
    "def infer_dtypes(path: Path, sample_rows: int = 10000) -> Dict[str, str]:\n",
    "    \"\"\"Infer low-memory dtypes from a small sample.\"\"\"\n",
    "    sample = pd.read_csv(path, nrows=sample_rows)\n",
    "    dtypes = {}\n",
    "    for col in sample.columns:\n",
    "        if col in [LABEL_COL, ATTACK_CAT]:\n",
    "            # Keep label as int8 if numeric; attack_cat stays object/categorical\n",
    "            if col == LABEL_COL and pd.api.types.is_numeric_dtype(sample[col]):\n",
    "                dtypes[col] = 'int8'\n",
    "            else:\n",
    "                dtypes[col] = 'object'\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_integer_dtype(sample[col]):\n",
    "            # Choose smallest adequate integer type\n",
    "            mn, mx = sample[col].min(), sample[col].max()\n",
    "            if mn >= 0:\n",
    "                if mx < 2**8: dtypes[col] = 'uint8'\n",
    "                elif mx < 2**16: dtypes[col] = 'uint16'\n",
    "                elif mx < 2**32: dtypes[col] = 'uint32'\n",
    "                else: dtypes[col] = 'uint64'\n",
    "            else:\n",
    "                if -128 <= mn <= 127 and -128 <= mx <= 127: dtypes[col] = 'int8'\n",
    "                elif -32768 <= mn <= 32767: dtypes[col] = 'int16'\n",
    "                elif -2147483648 <= mn <= 2147483647: dtypes[col] = 'int32'\n",
    "                else: dtypes[col] = 'int64'\n",
    "        elif pd.api.types.is_float_dtype(sample[col]):\n",
    "            dtypes[col] = 'float32'  # downcast\n",
    "        elif pd.api.types.is_bool_dtype(sample[col]):\n",
    "            dtypes[col] = 'bool'\n",
    "        else:\n",
    "            dtypes[col] = 'object'\n",
    "    return dtypes\n",
    "\n",
    "def get_columns(path: Path) -> List[str]:\n",
    "    return list(pd.read_csv(path, nrows=1).columns)\n",
    "\n",
    "def build_preprocessor(df_sample: pd.DataFrame) -> Tuple[Pipeline, List[str], List[str]]:\n",
    "    \"\"\"Create a ColumnTransformer for numeric+categorical with imputation+scaling/ordinal encoding.\"\"\"\n",
    "    X_cols = [c for c in df_sample.columns if c not in [LABEL_COL, ATTACK_CAT]]\n",
    "    numeric_cols = [c for c in X_cols if pd.api.types.is_numeric_dtype(df_sample[c])]\n",
    "    cat_cols = [c for c in X_cols if c not in numeric_cols]\n",
    "\n",
    "    numeric_pipe = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler(with_mean=True, with_std=True))\n",
    "    ])\n",
    "\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_pipe, numeric_cols),\n",
    "        ('cat', cat_pipe, cat_cols)\n",
    "    ], remainder='drop', n_jobs=N_JOBS)\n",
    "\n",
    "    return preprocessor, numeric_cols, cat_cols\n",
    "\n",
    "print(\"Preprocessing utilities ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec44a2",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We explore:\n",
    "\n",
    "- **Class distribution** (normal vs. attack)\n",
    "- **Attack category distribution**\n",
    "- **Feature correlations** (numeric subset)\n",
    "- **Feature importance** via a quick tree-based model on a sample (RandomForest)\n",
    "\n",
    "> Plots are produced on a representative sample to keep memory usage low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcb4c3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CSV not found; EDA will run after files are available.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def load_sample_for_eda(path: Path, n: int = EDA_SAMPLE_N) -> pd.DataFrame:\n",
    "    dtypes = infer_dtypes(path)\n",
    "    total = 0\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(path, dtype=dtypes, chunksize=50_000):\n",
    "        chunks.append(chunk)\n",
    "        total += len(chunk)\n",
    "        if total >= n:\n",
    "            break\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    # ensure label is int\n",
    "    if LABEL_COL in df.columns:\n",
    "        df[LABEL_COL] = pd.to_numeric(df[LABEL_COL], errors='coerce').fillna(0).astype('int8')\n",
    "    return df\n",
    "\n",
    "if TRAIN_CSV.exists():\n",
    "    eda_df = load_sample_for_eda(TRAIN_CSV, EDA_SAMPLE_N)\n",
    "    print(\"Sample for EDA:\", eda_df.shape)\n",
    "\n",
    "    # Class distribution\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=LABEL_COL, data=eda_df)\n",
    "    plt.title(\"Class Distribution (0=Normal, 1=Attack)\")\n",
    "    plt.xlabel(\"Label\"); plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    # Attack category distribution (if present)\n",
    "    if ATTACK_CAT in eda_df.columns:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        eda_df[ATTACK_CAT] = eda_df[ATTACK_CAT].astype('category')\n",
    "        eda_df[ATTACK_CAT].value_counts().head(15).plot(kind='bar')\n",
    "        plt.title(\"Top Attack Categories (Sample)\")\n",
    "        plt.xlabel(\"attack_cat\"); plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Correlations for numeric features\n",
    "    num_cols = [c for c in eda_df.columns if c not in [LABEL_COL, ATTACK_CAT] and pd.api.types.is_numeric_dtype(eda_df[c])]\n",
    "    corr = eda_df[num_cols + [LABEL_COL]].corr(numeric_only=True)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(corr, cmap='coolwarm', center=0)\n",
    "    plt.title(\"Feature Correlations (numeric subset)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Quick feature importance using RandomForest on sample\n",
    "    X_sample = eda_df.drop(columns=[LABEL_COL, ATTACK_CAT], errors='ignore')\n",
    "    y_sample = eda_df[LABEL_COL]\n",
    "    # Coerce non-numeric to category codes just for quick importance\n",
    "    for c in X_sample.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(X_sample[c]):\n",
    "            X_sample[c] = X_sample[c].astype('category').cat.codes\n",
    "    rf = RandomForestClassifier(n_estimators=100, n_jobs=N_JOBS, random_state=RANDOM_STATE)\n",
    "    rf.fit(X_sample, y_sample)\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_sample.columns).sort_values(ascending=False).head(20)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    importances.plot(kind='bar')\n",
    "    plt.title(\"Top 20 Feature Importances (RF on sample)\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training CSV not found; EDA will run after files are available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efde202",
   "metadata": {},
   "source": [
    "## 6. Model Building\n",
    "\n",
    "We implement two models:\n",
    "\n",
    "1. **Baseline:** Incremental Logistic Regression via `SGDClassifier(loss='log_loss')`\n",
    "   - Trained with **partial_fit** over chunks â†’ low memory footprint.\n",
    "2. **Optimized:** `HistGradientBoostingClassifier` (and optional XGBoost if installed)\n",
    "   - Trained on a capped number of rows for efficiency.\n",
    "   - Hyperparameters can be tuned via grid/random search as needed.\n",
    "\n",
    "We use a robust preprocessing `ColumnTransformer` for both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "335a2f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model utilities ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def stream_dataframe(path: Path, dtypes: Dict[str,str], chunksize: int = CHUNK_SIZE) -> Iterator[pd.DataFrame]:\n",
    "    for chunk in pd.read_csv(path, dtype=dtypes, chunksize=chunksize):\n",
    "        # Clean label\n",
    "        if LABEL_COL in chunk.columns:\n",
    "            chunk[LABEL_COL] = pd.to_numeric(chunk[LABEL_COL], errors='coerce').fillna(0).astype('int8')\n",
    "        yield chunk\n",
    "\n",
    "def fit_preprocessor_on_sample(path: Path, sample_rows: int = 100_000):\n",
    "    dtypes = infer_dtypes(path)\n",
    "    # gather sample\n",
    "    frames = []\n",
    "    total = 0\n",
    "    for chunk in stream_dataframe(path, dtypes, chunksize=50_000):\n",
    "        frames.append(chunk)\n",
    "        total += len(chunk)\n",
    "        if total >= sample_rows:\n",
    "            break\n",
    "    df_sample = pd.concat(frames, ignore_index=True)\n",
    "    pre, num_cols, cat_cols = build_preprocessor(df_sample)\n",
    "    # Fit on X sample only\n",
    "    Xs = df_sample.drop(columns=[LABEL_COL, ATTACK_CAT], errors='ignore')\n",
    "    pre.fit(Xs)\n",
    "    return pre, dtypes, num_cols, cat_cols\n",
    "\n",
    "def train_incremental_logreg(train_path: Path, preprocessor: ColumnTransformer, dtypes: Dict[str,str]) -> SGDClassifier:\n",
    "    clf = SGDClassifier(loss='log_loss', max_iter=5, learning_rate='optimal', random_state=RANDOM_STATE)\n",
    "    classes_ = np.array([0,1], dtype='int8')\n",
    "    for i, chunk in enumerate(stream_dataframe(train_path, dtypes)):\n",
    "        X_chunk = chunk.drop(columns=[LABEL_COL, ATTACK_CAT], errors='ignore')\n",
    "        y_chunk = chunk[LABEL_COL].values.astype('int8')\n",
    "        X_trans = preprocessor.transform(X_chunk)\n",
    "        if i == 0:\n",
    "            clf.partial_fit(X_trans, y_chunk, classes=classes_)\n",
    "        else:\n",
    "            clf.partial_fit(X_trans, y_chunk)\n",
    "        # free memory\n",
    "        del X_chunk, y_chunk, X_trans, chunk\n",
    "    return clf\n",
    "\n",
    "def load_cap(path: Path, dtypes: Dict[str,str], max_rows: Optional[int] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    frames = []\n",
    "    total = 0\n",
    "    for chunk in stream_dataframe(path, dtypes, chunksize=CHUNK_SIZE):\n",
    "        frames.append(chunk)\n",
    "        total += len(chunk)\n",
    "        if max_rows and total >= max_rows:\n",
    "            break\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    y = df[LABEL_COL].astype('int8')\n",
    "    X = df.drop(columns=[LABEL_COL, ATTACK_CAT], errors='ignore')\n",
    "    return X, y\n",
    "\n",
    "def train_hgb(train_path: Path, preprocessor: ColumnTransformer, dtypes: Dict[str,str], max_rows: Optional[int] = MAX_TRAIN_ROWS) -> HistGradientBoostingClassifier:\n",
    "    X_raw, y = load_cap(train_path, dtypes, max_rows=max_rows)\n",
    "    X = preprocessor.transform(X_raw)\n",
    "    hgb = HistGradientBoostingClassifier(max_depth=None, learning_rate=0.1, l2_regularization=0.0,\n",
    "                                         max_bins=255, random_state=RANDOM_STATE)\n",
    "    hgb.fit(X, y)\n",
    "    del X_raw, X, y\n",
    "    return hgb\n",
    "\n",
    "def try_train_xgboost(train_path: Path, preprocessor: ColumnTransformer, dtypes: Dict[str,str], max_rows: Optional[int] = MAX_TRAIN_ROWS):\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "    except Exception as e:\n",
    "        print(\"XGBoost not installed; skipping optimized XGB model.\")\n",
    "        return None\n",
    "    X_raw, y = load_cap(train_path, dtypes, max_rows=max_rows)\n",
    "    X = preprocessor.transform(X_raw)\n",
    "    dtrain = xgb.DMatrix(X, label=y)\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'eta': 0.1,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': RANDOM_STATE\n",
    "    }\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=200)\n",
    "    del X_raw, X, y, dtrain\n",
    "    return bst\n",
    "\n",
    "print(\"Model utilities ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c858ea",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "We evaluate each model on the official UNSW-NB15 **test** split with:\n",
    "\n",
    "- Accuracy, Precision, Recall, F1-score\n",
    "- Confusion Matrix\n",
    "- ROC Curve & AUC\n",
    "\n",
    "All computations are memory-conscious.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf0b31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_sklearn_model(model, preprocessor, test_path: Path, dtypes: Dict[str,str], model_name: str = \"Model\") -> Dict[str, Any]:\n",
    "    y_true_all, y_prob_all = [], []\n",
    "    for chunk in stream_dataframe(test_path, dtypes, chunksize=CHUNK_SIZE):\n",
    "        X_chunk = chunk.drop(columns=[LABEL_COL, ATTACK_CAT], errors='ignore')\n",
    "        y_chunk = chunk[LABEL_COL].astype('int8').values\n",
    "        X_trans = preprocessor.transform(X_chunk)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            prob = model.predict_proba(X_trans)[:,1]\n",
    "        else:\n",
    "            # SGDClassifier with 'log_loss' has decision_function\n",
    "            try:\n",
    "                from scipy.special import expit\n",
    "                prob = expit(model.decision_function(X_trans))\n",
    "            except Exception:\n",
    "                pred = model.predict(X_trans)\n",
    "                prob = pred  # fallback if needed\n",
    "        y_true_all.append(y_chunk)\n",
    "        y_prob_all.append(prob)\n",
    "        del X_chunk, y_chunk, X_trans, chunk\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_prob = np.concatenate(y_prob_all)\n",
    "    y_pred = (y_prob >= 0.5).astype('int8')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "    # Confusion matrix plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f\"Confusion Matrix - {model_name}\"); ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "    plt.show()\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(f\"ROC - {model_name}\"); plt.legend()\n",
    "    plt.show()\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"auc\": auc, \"cm\": cm}\n",
    "\n",
    "def evaluate_xgboost(bst, preprocessor, test_path: Path, dtypes: Dict[str,str]):\n",
    "    if bst is None:\n",
    "        return None\n",
    "    import xgboost as xgb\n",
    "    y_true_all, y_prob_all = [], []\n",
    "    for chunk in stream_dataframe(test_path, dtypes, chunksize=CHUNK_SIZE):\n",
    "        X_chunk = chunk.drop(columns=[LABEL_COL, ATTACK_CAT], errors='ignore')\n",
    "        y_chunk = chunk[LABEL_COL].astype('int8').values\n",
    "        X_trans = preprocessor.transform(X_chunk)\n",
    "        dtest = xgb.DMatrix(X_trans)\n",
    "        prob = bst.predict(dtest)\n",
    "        y_true_all.append(y_chunk)\n",
    "        y_prob_all.append(prob)\n",
    "        del X_chunk, y_chunk, X_trans, dtest, chunk\n",
    "    y_true = np.concatenate(y_true_all); y_prob = np.concatenate(y_prob_all)\n",
    "    y_pred = (y_prob >= 0.5).astype('int8')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    print(f\"\\n=== XGBoost (Optimized) ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}\")\n",
    "    # plots\n",
    "    import matplotlib.pyplot as plt\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix - XGBoost\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC (AUC={auc:.3f})\")\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC - XGBoost\"); plt.legend()\n",
    "    plt.show()\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"auc\": auc, \"cm\": cm}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb67c09",
   "metadata": {},
   "source": [
    "### 6.1 Train & Evaluate\n",
    "\n",
    "We fit the preprocessing on a representative sample, then:\n",
    "\n",
    "- Train **incremental logistic regression** over chunks\n",
    "- Train **HistGradientBoosting** on a capped number of rows\n",
    "- Optionally train **XGBoost** if available\n",
    "- Evaluate all on the **test** split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2f63a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/testing CSVs not found. Place them in ./data_unsw_nb15 and re-run.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if TRAIN_CSV.exists() and TEST_CSV.exists():\n",
    "    preprocessor, dtypes, num_cols, cat_cols = fit_preprocessor_on_sample(TRAIN_CSV, sample_rows=150_000)\n",
    "\n",
    "    # Baseline: Incremental Logistic Regression\n",
    "    sgd_clf = train_incremental_logreg(TRAIN_CSV, preprocessor, dtypes)\n",
    "    results_sgd = evaluate_sklearn_model(sgd_clf, preprocessor, TEST_CSV, dtypes, model_name=\"Baseline SGD-LogReg\")\n",
    "\n",
    "    # Optimized: HistGradientBoosting\n",
    "    hgb_clf = train_hgb(TRAIN_CSV, preprocessor, dtypes, max_rows=MAX_TRAIN_ROWS)\n",
    "    results_hgb = evaluate_sklearn_model(hgb_clf, preprocessor, TEST_CSV, dtypes, model_name=\"Optimized HistGB\")\n",
    "\n",
    "    # Optional: XGBoost\n",
    "    bst = try_train_xgboost(TRAIN_CSV, preprocessor, dtypes, max_rows=MAX_TRAIN_ROWS)\n",
    "    results_xgb = evaluate_xgboost(bst, preprocessor, TEST_CSV, dtypes)\n",
    "else:\n",
    "    print(\"Training/testing CSVs not found. Place them in ./data_unsw_nb15 and re-run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db6c49",
   "metadata": {},
   "source": [
    "## 8. Centralized Log Management Simulation\n",
    "\n",
    "We simulate logs being ingested from multiple sources (e.g., **system**, **application**, **network**) into one pipeline.\n",
    "The model processes incoming events and emits alerts for suspicious (attack-labeled) traffic.\n",
    "\n",
    "This section demonstrates:\n",
    "\n",
    "- A unified **log collector** that yields records from sources\n",
    "- Preprocessing + model inference in streaming fashion\n",
    "- Thresholded **alerting** with latency-friendly batching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b85b314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files not present; simulation will run once data is available.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from itertools import cycle\n",
    "\n",
    "def synthetic_source_names() -> List[str]:\n",
    "    return [\"syslog\", \"app\", \"netflow\"]\n",
    "\n",
    "def multi_source_stream(paths: List[Path], dtypes: Dict[str,str], batch_size: int = 5000) -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"Round-robin across paths to simulate centralized ingestion.\"\"\"\n",
    "    iters = [pd.read_csv(p, dtype=dtypes, chunksize=batch_size) for p in paths]\n",
    "    for it, name in zip(iters, cycle(synthetic_source_names())):\n",
    "        try:\n",
    "            chunk = next(it)\n",
    "            chunk[\"_source\"] = name\n",
    "            yield chunk\n",
    "        except StopIteration:\n",
    "            continue\n",
    "\n",
    "def realtime_scoring(pipeline_pre, model, stream_it: Iterator[pd.DataFrame], alert_threshold: float = 0.8, max_batches: int = 10):\n",
    "    alerts = []\n",
    "    for i, batch in enumerate(stream_it):\n",
    "        Xb = batch.drop(columns=[LABEL_COL, ATTACK_CAT], errors='ignore')\n",
    "        Xb_trans = pipeline_pre.transform(Xb)\n",
    "        # probability\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            prob = model.predict_proba(Xb_trans)[:,1]\n",
    "        else:\n",
    "            from scipy.special import expit\n",
    "            prob = expit(model.decision_function(Xb_trans))\n",
    "        flagged = batch[prob >= alert_threshold].copy()\n",
    "        flagged[\"threat_score\"] = prob[prob >= alert_threshold]\n",
    "        for _, row in flagged.iterrows():\n",
    "            alerts.append({\n",
    "                \"ts\": time.time(),\n",
    "                \"source\": row.get(\"_source\", \"unknown\"),\n",
    "                \"score\": float(row[\"threat_score\"]),\n",
    "                \"summary\": \"High-risk event detected\"\n",
    "            })\n",
    "        print(f\"Batch {i+1}: processed {len(batch)} events, alerts: {len(flagged)}\")\n",
    "        if i+1 >= max_batches:\n",
    "            break\n",
    "    return alerts\n",
    "\n",
    "# Demo (only runs if files exist and a trained model is available)\n",
    "if TRAIN_CSV.exists() and TEST_CSV.exists():\n",
    "    try:\n",
    "        alerts = realtime_scoring(preprocessor, hgb_clf, multi_source_stream([TEST_CSV], dtypes, batch_size=10_000), alert_threshold=0.9, max_batches=5)\n",
    "        print(f\"Total alerts emitted: {len(alerts)}\\nSample:\")\n",
    "        print(alerts[:5])\n",
    "    except NameError:\n",
    "        print(\"Train models first (previous cell) to run the real-time simulation.\")\n",
    "else:\n",
    "    print(\"CSV files not present; simulation will run once data is available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08363d29",
   "metadata": {},
   "source": [
    "## 9. Conclusion & Future Work\n",
    "\n",
    "We built a memory-conscious, production-minded CLMS pipeline on **UNSW-NB15** featuring:\n",
    "\n",
    "- Robust ingestion and dtype-optimized preprocessing with chunked loading\n",
    "- Clear EDA and interpretability touchpoints (correlations, feature importance)\n",
    "- Two detection models: **incremental logistic regression** and **histogram-based GBM**\n",
    "- Full evaluation (Accuracy/Precision/Recall/F1/AUC)\n",
    "- Real-time simulation with centralized ingestion and alerting\n",
    "\n",
    "**Future enhancements:**\n",
    "\n",
    "- Enrich features with time-window aggregations and entity profiling (e.g., per host/user)\n",
    "- Add drift detection and online calibration for thresholds\n",
    "- Integrate with message queues (Kafka), monitoring (Prometheus), and SIEM systems\n",
    "- Expand to multi-class detection over `attack_cat` or hierarchical detectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa3d47",
   "metadata": {},
   "source": [
    "### Appendix: Memory Tips\n",
    "\n",
    "- Reduce `CHUNK_SIZE` if you run low on RAM.\n",
    "- Lower `MAX_TRAIN_ROWS` for the optimized model.\n",
    "- Use swap or smaller samples for EDA.\n",
    "- Ensure you're running a 64-bit Python with enough available memory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
